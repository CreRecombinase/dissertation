#+TITLE: Functional Enrichment by Expectation Maximization
#+SETUPFILE: setup.org
#+OPTIONS: toc:2 |:t

* Introduction

When interpreting the results of a genome-scale genetic association study, be it a GWAS, a whole exome sequencing study, or a whole genome sequencing study, there are two questions that are of interest:
1) which genes are associated with the trait of interest, and 2) what are the common properties of the genes that contribute to disease risk. The answer to one of these questions can inform the other.  
Obviously, identifying a set of properties common to causal genes is made much easier if the set of causal genes is known with certainty. Furthermore, if the set of causal genes is not known,
prior knowledge about the properties of the causal genes is available, that prior knowledge can be used to implicate genes that might otherwise not have been identified due to a lack of statistical power, or
similarly to deprioritize genes that might otherwise be implicated, despite being false positives.  Despite the complementarity of these 




* Method

** Model

Suppose we have gathered genetic data for a set of individuals to identify which genes are causally related to a disease or trait of interest.  For each gene $g \in \{1 \dots G\}$, let the indicator variable $z_g=1$ indicate that
gene $g$ is causaully related to our trait or disease of interest.  We can summarise the evidence for and against our hypothesis that $z_g=1$ using a bayes factor:

$$B_g=\frac{P(x_g|z_g=1)}{P(x_g|z_g=0)}$$

where $x_g$ is the subset of the aforementioned genetic data corresponding to the $g$th gene.

Suppose further that we know a set of $F$ functional annotations or properties for each of our $G$ genes.  Let $\textbf{a}_g$ denote the length $F$ vector of annotations for gene $g$, and $\textbf{A}$ denote the matrix with $F$ rows and $G$ 
columns consisting of $\textbf{a}_1 ...  \textbf{a}_G$
We define the vector $\boldsymbol{\beta}$ and the function $\pi(\boldsymbol{\beta},\textbf{a}_g)$ such that:

$$P\pi(\boldsymbol{\beta},\textbf{a}_g) =  \frac{1}{1+e^{-(\beta_{0}+\sum_{f=1}^F{A_{f,g}\beta_f})}} =  (z_g=1|\textbf{a}_g,\boldsymbol{\beta})$$

We can compute the likelihood of a particular value of $\boldsymbol{\beta}$ as:

$$ P(\textbf{x}|\boldsymbol{\beta},\textbf{A})=\prod_{g=1}^{G}P(x_g|\boldsymbol{\beta})=\prod_{g=1}^{G}[\pi(\boldsymbol{\beta},\textbf{a}_g) P(x_g|z_g=1)+(1-\pi(\boldsymbol{\beta},\textbf{a}_g))P(x_g|z_g=0)]$$

By factorizing out the term $\prod_{g=1}^{G} P(x_g|z_g=0)$ which does not depend on $\boldsymbol{\beta}$, we can express the likelihood for $\boldsymbol{\beta}$ in terms of $\textbf{B}$:


$$P(\textbf{x}|\boldsymbol{\beta},\textbf{A}) \propto \prod_{g=1}^{G}[\pi(\boldsymbol{\beta},\textbf{a}_g)B_g+(1-\pi(\boldsymbol{\beta},\textbf{a}_g))]$$


Given a particular value of $\boldsymbol{\beta}$, we arrive at a new posterior:

$$P(z_g=1 | x_g, \boldsymbol{\beta},\textbf{a}_g) = \frac{\pi(\boldsymbol{\beta},\textbf{a}_g) B_g}{\pi(\boldsymbol{\beta} , \textbf{a}_g) B_g + 1 - \pi(\boldsymbol{\beta},\textbf{a}_g)}$$


** Algorithm 
 
Our goal is both to estimate $\boldsymbol{\beta}$ as well as $P(Z_g=1|\textbf{a}_g,\boldsymbol{\beta},x_g)$ for each gene.  We use the EM algorithm cite:EM_algo to
obtain a maximum likelihood estimate for $\boldsymbol{\beta}$.  If $\textbf{z}$ is our latent variable then our complete data likelihood is:

$$P(\textbf{x},\textbf{z}|\boldsymbol{\beta},\textbf{A}) = 
\prod_{g=1}^G \left[P(z_g|\boldsymbol{\beta},\textbf{a}_g) P(x_g|z_g)\right] = 
\prod_{g=1}^G \left[\pi(\boldsymbol{\beta},\textbf{a}_g)^{z_g} (1-\pi(\boldsymbol{\beta},\textbf{a}_g))^{1-z_g}B_g^{z_g}\right]$$ 

From which we form the $Q$ function

$$ Q(\boldsymbol{\beta}|\boldsymbol{\beta}_n) = \sum_{g=1}^G \left[P(z_g=1|x_g,\boldsymbol{\beta}_n,\textbf{a}_g) \log\left(P(x_g,z_g=1|\boldsymbol{\beta})\right) +
P(z_g=0|x_g,\boldsymbol{\beta}_n,\textbf{a}_g) \log\left(P(x_g,z_g=0|\boldsymbol{\beta})\right) \right] = 
\sum_{g=1}^G \left [u_g(\log(\pi(\boldsymbol{\beta_n},\textbf{a}_g))+B_g)+(1-u_g)\log(1-\pi(\boldsymbol{\beta_n},\textbf{a}_g)) \right]$$

Where $u_g=P(z_g=1|x_g,\boldsymbol{\beta_n},\textbf{a}_g) = \frac{\pi(\boldsymbol{\beta_n},\textbf{a}_g)B_g}{\pi(\boldsymbol{\beta_n},\textbf{a}_g)B_g+1-\pi(\boldsymbol{\beta_n},\textbf{a}_g)}$

The model fitting procedure proceeds as follows 

+ Start with an initial guess of $\boldsymbol{\beta}$
+ Repeat until $Q(\boldsymbol{\beta}|\boldsymbol{\beta}_{n-1})-Q(\boldsymbol{\beta}|\boldsymbol{\beta}_n)< \text{tol}$:
  - Compute $P(z_g=1|x_g,\boldsymbol{\beta_n},\textbf{a}_g)$
  - Maximize the likelihood for $\boldsymbol{\beta}$ using proportional logistic regression

** Algorithm (penalized Maximum Likelihood with BFGS)

An alternative to the Expectation Maximization framework is a direct, maximum-likelihood approach.  The Limited Memory Broyden-Fletcher-Goldfarb-Shanno algorithm (LM-BFGS) cite:LMBFGS  is among the most popular algorithms 
for unconstrained optimization over scalar, differentiable functions. 

One limitation of LM-BFGS, is that, as mentioned previously, the function that is being optimized must be differentiable.  Unfortunately, sparsity-inducing $l_1$-regularized models of the form

$$f(\boldsymbol{\theta})=p(\boldsymbol{\theta} | \textbf{x}) + C \Vert \boldsymbol{\theta} \Vert_1$$ are not differentiable when any of the elements of the parameter vector $\boldsybmol{\theta}$ are 0.  The Orthant-wise limited-memory quasi-Newton method is a variant of LM-BFGS which is designed precisely for 
fitting $l_1$-regularized, sparsity inducing models. 


*** Feature selection (Forward Selection)

The number of gene-level features one might include in such a model is very large.  It is impossible, from both a computability 
and interpretability standpoint, to include all conceivable features in the model. A related but distinct issue is that of collinearity.
As the number of features in the model increases, there is a higher probability that some subset of features will be collinear with one-another,
which can complicate model-fitting.  To avoid this issue, we employ a multi-stage model fitting procedure. In the first step, all single-feature-plus-intercept
models are fit, and their $p$-value 

We employed a forward selection procedure to construct a multivariate model.  We begin the procedure by 
fitting all features in single-feature models.  We test the significance of each model against an intercept-only model using the likelihood ratio test.
From this set of univariate models multivariate models all of the nonsignificant (false discovery rate of 0.05) univariate features for each cancer type
 were removed from the analysis.  In the case of KICH, no features were significant after multiple testing correction (minimum $q$ value: 0.104 correspodning to GO:0071456, "cellular response to hypoxia"), and in the case of CHOL,
only one feature was significant after multiple testing correction (GO:0008285 " negative regulation of cell population proliferation", $q$ value: 0.00909). 
Significant features were then 
further pruned using a jaccard index similarity cutoff.

** Jaccard index for binary feature overlap

The Jaccard index that measures the similarity of two sets.  GO terms are binary features (gene is either a member of a GO term or it is not), and can be 
models as a set, where the genes in the GO terms are the elements of the set. The definition of the Jaccard Index is:

$$ J(A,B) = \frac{|A \cap B |}{A \cup B} = \frac{| A \cap B|}{|A|+|B|-|A \cap B|} $$

The strategy for feature selection works as follows: first take the most significant single-feature model,
and then remove the most similar features to the selected feature (i.e the features with a jaccard similarity above 0.1).
Then fit all 2 term models that include the most significant feature, select the feature with the highest significance when tested against the single 
feature model, and remove all features similar to the features in the selected model. This process of testing all available $n$ feature models, against the
(greedily) best $n-1$-feature model, and removing from the candidate pool features similar to the chosen $n$th feature, is repeated until the most significant model in the $n$ term vs
$n-1$ comparison is not significant at $p<0.05$ by the likelihood ratio test.


** Validation against COSMIC Cancer Gene Census

The Catalogue of Somatic Mutations in Cancer (COSMIC) Cancer Gene Census (CGC) cite:COSMIC is an effort to catalogue genes which contain mutations
causally implicated in cancer. To date, the CGC has identified  To validate the FGEM models, CGC genes were used as a gold standard.    

** Validation against 

** Data 


*** Prognostic 

The Human Pathology Atlas is a dataset of 900,000 patient survival profiles across 17 types of cancersurvival data cite:uhlen17_pathol_atlas_human_cancer_trans

*** Gene-Level Summary Statistics

A set of Bayes factors from a study of cancer driver genes cite:zhao19_detail_model_posit_selec_improv.  For each of 20 TCGA tumor types, roughly 20,000 genes were analyzed and the 
posterior probability that each gene (in each cancer type) was a causal gene was assessed and summarized via Bayes Factor.  

*** Gene-Level Annotations
    
**** Gene Ontology 

The "Biological Process" Gene Ontology cite:GO  was downloaded from the Bioconductor package ~GO.db~ cite:godb. Of the 10,930 possible biological process gene ontology terms, the 2,198 terms that 
include 10 or more genes were analyzed, so as to reduce the multiple testing burden.  


** Software

Our method is distributed as a freely available R package [[https://github.com/CreRecombinase/FGEM][FGEM]] cite:R .FGEM relies on the ~SQUAREM~ package to accelerate EM convergence  cite:squarem   
and the hot-path functions are implemented as C++ functions using the ~RcppArmadillo~ cite:RcppArmadillo package.  


* Results


** COSMIC validation

The cross-entropy of the FGEM model for each cancer type was compared to the cross-entropy of the intercept-only
FGEM model using the COSMIC census genes as ground truth.  For each of the 20 cancer types FGEM had lower cross-entropy 
(higher likelihood ratio)


| cancer | cross entropy functional | cross entropy uniform | cross entropy ratio |
|--------+--------------------------+-----------------------+---------------------|
| BLCA   |        -95662.4678891365 |      -85246.945544574 |   -10415.5223445625 |
| BRCA   |        -118449.351908682 |     -90864.7631741114 |   -27584.5887345709 |
| CESC   |        -147468.020375775 |     -79847.4862860612 |   -67620.5340897143 |
| CHOL   |        -54147.2464991959 |     -50321.7386081126 |   -3825.50789108334 |
| ESCA   |         -118908.27548448 |     -84631.9895615394 |   -34276.2859229405 |
| GBM    |        -92362.5624198571 |     -73787.0866635762 |   -18575.4757562809 |
| HNSC   |        -141820.887516856 |      -97862.081096597 |    -43958.806420259 |
| KICH   |        -109133.000058618 |     -96625.4414397593 |   -12507.5586188592 |
| KIRC   |        -101543.318511605 |     -80891.7016637173 |   -20651.6168478874 |
| KIRP   |        -95512.8357907782 |     -84980.3471977415 |   -10532.4885930367 |
| LIHC   |        -110051.739122586 |     -91896.1246808558 |   -18155.6144417298 |
| LUAD   |        -177084.209088335 |       -115584.2079699 |   -61500.0011184357 |
| LUSC   |        -157500.378816593 |     -106769.242183055 |   -50731.1366335384 |
| PAAD   |        -318932.227641074 |     -66508.8323704442 |    -252423.39527063 |
| PRAD   |        -891940.514321957 |     -76779.4406090754 |   -815161.073712881 |
| SARC   |        -636965.982410518 |     -81431.9753511472 |   -555534.007059371 |
| SKCM   |        -133127.342562248 |     -121032.432355748 |   -12094.9102065007 |
| TGCT   |        -760084.365698844 |     -105591.835808511 |   -654492.529890333 |
| UCEC   |        -111681.722486175 |     -86440.4633034317 |   -25241.2591827432 |
| UCS    |        -99619.9782423041 |     -81147.3017795327 |   -18472.6764627714 |


* Discussion






bibliographystyle:unsrt
bibliography:references.bib







