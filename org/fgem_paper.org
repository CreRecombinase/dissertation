#+TITLE: Functional Enrichment by Expectation Maximization
#+SETUPFILE: setup.org
#+OPTIONS: toc:2 |:t

* Introduction

When interpreting the results of a genome-scale genetic association study, be it a GWAS, a whole exome sequencing study, or a whole genome sequencing study, there are two questions that are of interest:
1) which genes are associated with the trait of interest, and 2) what are the common properties of the genes that contribute to disease risk. The answer to one of these questions can inform the other.  
Obviously, identifying a set of properties common to causal genes is made much easier if the set of causal genes is known with certainty. Furthermore, if the set of causal genes is not known,
prior knowledge about the properties of the causal genes is available, that prior knowledge can be used to implicate genes that might otherwise not have been identified due to a lack of statistical power, or
similarly to deprioritize genes that might otherwise be implicated, despite being false positives.  Despite the complementarity of these 




* Method

** Model

Suppose we have gathered genetic data for a set of individuals to identify which genes are causally related to a disease or trait of interest. For each gene $g \in \{1 \dots G\}$, let the indicator variable $z_g=1$ indicate that
gene $g$ is causaully related to our trait or disease of interest.  We can summarise the evidence for and against our hypothesis that $z_g=1$ using a bayes factor:

$$B_g=\frac{P(x_g|z_g=1)}{P(x_g|z_g=0)}$$

where $x_g$ is the subset of the aforementioned genetic data corresponding to the $g$th gene.

Suppose further that we know a set of $F$ functional annotations or properties for each of our $G$ genes.  Let $\textbf{a}_g$ denote the length $F$ vector of annotations for gene $g$, and $\textbf{A}$ denote the matrix with $F$ rows and $G$ 
columns consisting of $\textbf{a}_1 ...  \textbf{a}_G$
We define the vector $\boldsymbol{\beta}$ and the function $\pi(\boldsymbol{\beta},\textbf{a}_g)$ such that:

$$\pi(\boldsymbol{\beta},\textbf{a}_g) =  \frac{1}{1+e^{-(\beta_{0}+\sum_{f=1}^F{A_{f,g}\beta_f})}} =  P(z_g=1|\textbf{a}_g,\boldsymbol{\beta})$$

We can compute the likelihood of a particular value of $\boldsymbol{\beta}$ by treating the data from each gene as coming from a two component mixture model (where $z_g=1$ and where $z_g=0$) and marginalizing over the two components

$$ P(\textbf{x}|\boldsymbol{\beta},\textbf{A})=\prod_{g=1}^{G}P(x_g|\boldsymbol{\beta})=\prod_{g=1}^{G}[\pi(\boldsymbol{\beta},\textbf{a}_g) P(x_g|z_g=1)+(1-\pi(\boldsymbol{\beta},\textbf{a}_g))P(x_g|z_g=0)]$$

By factorizing out the term $\prod_{g=1}^{G} P(x_g|z_g=0)$ which does not depend on $\boldsymbol{\beta}$, we can express the likelihood for $\boldsymbol{\beta}$ in terms of $\textbf{B}$:


$$P(\textbf{x}|\boldsymbol{\beta},\textbf{A}) \propto \prod_{g=1}^{G}[\pi(\boldsymbol{\beta},\textbf{a}_g)B_g+(1-\pi(\boldsymbol{\beta},\textbf{a}_g))]$$


Given a particular value of $\boldsymbol{\beta}$, and a bayes factor $B_g$, we arrive at a new posterior probability that $z_g=1$:

$$P(z_g=1 | B_g, \boldsymbol{\beta},\textbf{a}_g) = \frac{\pi(\boldsymbol{\beta},\textbf{a}_g) B_g}{\pi(\boldsymbol{\beta} , \textbf{a}_g) B_g + 1 - \pi(\boldsymbol{\beta},\textbf{a}_g)}$$

 
Our goal is both to estimate $\boldsymbol{\beta}$ for a relevant set of features as well as $P(Z_g=1|\textbf{a}_g,\boldsymbol{\beta},x_g)$ for each gene.  We use a penalized maximum-likelihood
 approach to estimate $\boldsymbol{\beta}$, from which we then $P(Z_g=1|\textbf{a}_g,\boldsymbol{\beta},x_g)$. The number of gene-level features one might include in such a model is very large.  It is impossible, from both a computability 
and interpretability standpoint, to include all conceivable features in the model. A related but distinct issue is that of collinearity.
As the number of features in the model increases, the probability that some subset of features will be collinear with one-another increases,
which can complicate model-fitting, as $\beta$ becomes unidentifiable. This is especially important when a binary, hierarchical feature set like the Gene Ontology.To avoid this issue, 
we employ a multi-stage model fitting procedure. In the first step, all single-feature-plus-intercept
models are fit, and a $p$-value is obtained for each model byt comparing to the intercept-only model via the likelihood ratio test.  

From this set of single-feature models, all of the nonsignificant (i.e features with Benjamini-Hochberg adjusted $p$-values greater than 0.05) univariate features for each cancer type
were removed from the analysis.  In the case of KICH, no features were significant after multiple testing correction (minimum $q$ value: 0.104 correspodning to GO:0071456, "cellular response to hypoxia"), and in the case of CHOL,
only one feature was significant after multiple testing correction (GO:0008285 " negative regulation of cell population proliferation", $q$ value: 0.00909). 


Significant features for each cancer type were then combined in a joint model and fit with 

The Limited Memory Broyden-Fletcher-Goldfarb-Shanno algorithm (LM-BFGS) cite:LMBFGS  is among the most popular algorithms 
for unconstrained optimization over scalar, differentiable functions. 

One limitation of LM-BFGS, is that, as mentioned previously, the function that is being optimized must be differentiable.  Unfortunately, sparsity-inducing $l_1$-regularized models of the form

$$f(\boldsymbol{\theta})=p(\boldsymbol{\theta} | \textbf{x}) + C \Vert \boldsymbol{\theta} \Vert_1$$ are not differentiable when any of the elements of the parameter vector $\boldsybmol{\theta}$ are 0.  The Orthant-wise limited-memory quasi-Newton method is a variant of LM-BFGS which is designed precisely for 
fitting $l_1$-regularized, sparsity inducing models. 

As the number of features in 


If $\textbf{z}$ is our latent variable then our complete data likelihood is:
$$P(\textbf{x},\textbf{z}|\boldsymbol{\beta},\textbf{A}) = 
\prod_{g=1}^G \left[P(z_g|\boldsymbol{\beta},\textbf{a}_g) P(x_g|z_g)\right] = 
\prod_{g=1}^G \left[\pi(\boldsymbol{\beta},\textbf{a}_g)^{z_g} (1-\pi(\boldsymbol{\beta},\textbf{a}_g))^{1-z_g}B_g^{z_g}\right]$$ 

From which we form the $Q$ function

$$ Q(\boldsymbol{\beta}|\boldsymbol{\beta}_n) = \sum_{g=1}^G \left[P(z_g=1|x_g,\boldsymbol{\beta}_n,\textbf{a}_g) \log\left(P(x_g,z_g=1|\boldsymbol{\beta})\right) +
P(z_g=0|x_g,\boldsymbol{\beta}_n,\textbf{a}_g) \log\left(P(x_g,z_g=0|\boldsymbol{\beta})\right) \right] = 
\sum_{g=1}^G \left [u_g(\log(\pi(\boldsymbol{\beta_n},\textbf{a}_g))+B_g)+(1-u_g)\log(1-\pi(\boldsymbol{\beta_n},\textbf{a}_g)) \right]$$

Where $u_g=P(z_g=1|x_g,\boldsymbol{\beta_n},\textbf{a}_g) = \frac{\pi(\boldsymbol{\beta_n},\textbf{a}_g)B_g}{\pi(\boldsymbol{\beta_n},\textbf{a}_g)B_g+1-\pi(\boldsymbol{\beta_n},\textbf{a}_g)}$

The model fitting procedure proceeds as follows 

+ Start with an initial guess of $\boldsymbol{\beta}$
+ Repeat until $Q(\boldsymbol{\beta}|\boldsymbol{\beta}_{n-1})-Q(\boldsymbol{\beta}|\boldsymbol{\beta}_n)< \text{tol}$:
  - Compute $P(z_g=1|x_g,\boldsymbol{\beta_n},\textbf{a}_g)$
  - Maximize the likelihood for $\boldsymbol{\beta}$ using proportional logistic regression


** Validation against intOGen

IntOGen is a database of cancer driver genes cite:gonzalez-perez13_intog_mutat_ident_cancer_driver.  It is populated by an ensembl method that incorporates seven different methods for identifying cancer driver genes.  
It weights each of the 7 methods according to their ability to predict membership in the The Catalogue of Somatic Mutations in Cancer (COSMIC) Cancer Gene Census (CGC) cite:COSMIC.
To validate the FGEM models, 

** Validation against 

** Data 


*** Prognostic 

The Human Pathology Atlas is a dataset of 900,000 patient survival profiles across 17 types of cancersurvival data cite:uhlen17_pathol_atlas_human_cancer_trans

*** Gene-Level Summary Statistics

A set of Bayes factors from a study of cancer driver genes cite:zhao19_detail_model_posit_selec_improv.  For each of 20 TCGA tumor types, roughly 20,000 genes were analyzed and the 
posterior probability that each gene (in each cancer type) was a causal gene was assessed and summarized via Bayes Factor.  

*** Gene-Level Annotations
    
**** Gene Ontology 

The "Biological Process" Gene Ontology cite:GO  was downloaded from the Bioconductor package ~GO.db~ cite:godb. Of the 10,930 possible biological process gene ontology terms, the 2,198 terms that 
include 10 or more genes were analyzed, so as to reduce the multiple testing burden.  

**** Molecular Signatures Database

The Molecular Signatures Database (MSigDB) cite:msigdb is among the most popular collections of annotated gene sets. Originally
deveoped for the method Gene Set Enrichment Analysis (GSEA) cite:Subramanian_2005, it contains over 25000 gene sets across 
8 major collections.  

These collections are :
1) Hallmark (H) gene sets represent "well-defined biological states or processses and display coherent expression"
2) Positional (C1) gene sets correspond to each of the human chromosomes and cytogenic bands that contain at least one gene. 
3) Curated (C2) gene sets are manually curated gene sets from various databases and biomedical publications.  C2 includes gene sets 
from: CGP cite:CGP , BIOCARTA cite:BIOCARTA, KEGG cite:KEGG, PID, cite:PID and REACTOME cite:REACTOME.  
4) Regulatory gene sets  (C3)
5) Computational gene sets (C4)
6) GO gene sets (C5)
7) oncogenic gene sets (C6)
8) immunologic gene sets (C7)


** Software

Our method is distributed as a freely available R package [[https://github.com/CreRecombinase/FGEM][FGEM]] cite:R .FGEM relies on the ~SQUAREM~ package to accelerate EM convergence  cite:squarem   
and the hot-path functions are implemented as C++ functions using the ~RcppArmadillo~ cite:RcppArmadillo package.  


* Results


** COSMIC validation

The cross-entropy of the FGEM model for each cancer type was compared to the cross-entropy of the intercept-only
FGEM model using the COSMIC census genes as ground truth.  For each of the 20 cancer types FGEM had lower cross-entropy 
(higher likelihood ratio)


| cancer | cross entropy functional | cross entropy uniform | cross entropy ratio |
|--------+--------------------------+-----------------------+---------------------|
| BLCA   |        -95662.4678891365 |      -85246.945544574 |   -10415.5223445625 |
| BRCA   |        -118449.351908682 |     -90864.7631741114 |   -27584.5887345709 |
| CESC   |        -147468.020375775 |     -79847.4862860612 |   -67620.5340897143 |
| CHOL   |        -54147.2464991959 |     -50321.7386081126 |   -3825.50789108334 |
| ESCA   |         -118908.27548448 |     -84631.9895615394 |   -34276.2859229405 |
| GBM    |        -92362.5624198571 |     -73787.0866635762 |   -18575.4757562809 |
| HNSC   |        -141820.887516856 |      -97862.081096597 |    -43958.806420259 |
| KICH   |        -109133.000058618 |     -96625.4414397593 |   -12507.5586188592 |
| KIRC   |        -101543.318511605 |     -80891.7016637173 |   -20651.6168478874 |
| KIRP   |        -95512.8357907782 |     -84980.3471977415 |   -10532.4885930367 |
| LIHC   |        -110051.739122586 |     -91896.1246808558 |   -18155.6144417298 |
| LUAD   |        -177084.209088335 |       -115584.2079699 |   -61500.0011184357 |
| LUSC   |        -157500.378816593 |     -106769.242183055 |   -50731.1366335384 |
| PAAD   |        -318932.227641074 |     -66508.8323704442 |    -252423.39527063 |
| PRAD   |        -891940.514321957 |     -76779.4406090754 |   -815161.073712881 |
| SARC   |        -636965.982410518 |     -81431.9753511472 |   -555534.007059371 |
| SKCM   |        -133127.342562248 |     -121032.432355748 |   -12094.9102065007 |
| TGCT   |        -760084.365698844 |     -105591.835808511 |   -654492.529890333 |
| UCEC   |        -111681.722486175 |     -86440.4633034317 |   -25241.2591827432 |
| UCS    |        -99619.9782423041 |     -81147.3017795327 |   -18472.6764627714 |


* Discussion






bibliographystyle:unsrt
bibliography:references.bib







